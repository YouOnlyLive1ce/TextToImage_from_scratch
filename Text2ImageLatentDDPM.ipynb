{"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"widgets":{"application/vnd.jupyter.widget-state+json":{"cbac743b2cb44e0b9881a98a23de6317":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_37afc165b7334a049e8e50983c188f7b","IPY_MODEL_d22d9f1a7c4c4ef5bd11086ec7a69dc4","IPY_MODEL_1cc04a92d8884d5b8081195bfc2f4cc4"],"layout":"IPY_MODEL_30cc8298bda444e8a1485bb2da0bf2c6"}},"37afc165b7334a049e8e50983c188f7b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8f5e9e406e3f47bb8c7aad278e67db79","placeholder":"​","style":"IPY_MODEL_66ae1b5f7189418b906553ec16771f8c","value":"Epoch 0: train:   0%"}},"d22d9f1a7c4c4ef5bd11086ec7a69dc4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_19b201c040c0410e8eddfca6de6788fe","max":101300,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81a57e9b24e74df8b29b2549a59f5750","value":0}},"1cc04a92d8884d5b8081195bfc2f4cc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a8e6d4f457c8446691a41832c9f594f9","placeholder":"​","style":"IPY_MODEL_eeeed015ee8640cd9b75db39e906bc88","value":" 0/101300 [00:00&lt;?, ?it/s]"}},"30cc8298bda444e8a1485bb2da0bf2c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8f5e9e406e3f47bb8c7aad278e67db79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66ae1b5f7189418b906553ec16771f8c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"19b201c040c0410e8eddfca6de6788fe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81a57e9b24e74df8b29b2549a59f5750":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"a8e6d4f457c8446691a41832c9f594f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eeeed015ee8640cd9b75db39e906bc88":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":37705,"datasetId":29561,"databundleVersionId":39327}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Dataset manipulations","metadata":{"id":"OXdwA73ALs6E"}},{"cell_type":"code","source":"!pip install torch==2.3.0\n!pip install torchvision==0.18.0\n!pip install torchtext==0.18.0\n!pip install kagglehub","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBrLHwTjcpVJ","outputId":"8cd653fc-3efc-4ee7-b305-e1a41b0d15ef","execution":{"iopub.status.busy":"2024-11-16T17:54:05.559660Z","iopub.execute_input":"2024-11-16T17:54:05.560033Z","iopub.status.idle":"2024-11-16T17:57:02.004390Z","shell.execute_reply.started":"2024-11-16T17:54:05.559999Z","shell.execute_reply":"2024-11-16T17:57:02.003238Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Collecting torch==2.3.0\n  Downloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl.metadata (26 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0) (2024.6.1)\nCollecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.3.0)\n  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.3.0)\n  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.3.0)\n  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.2.106 (from torch==2.3.0)\n  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.3.0)\n  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.3.0)\n  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-nccl-cu12==2.20.5 (from torch==2.3.0)\n  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\nCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.3.0)\n  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\nCollecting triton==2.3.0 (from torch==2.3.0)\n  Downloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\nCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0)\n  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0) (1.3.0)\nDownloading torch-2.3.0-cp310-cp310-manylinux1_x86_64.whl (779.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m779.1/779.1 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m75.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading triton-2.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (168.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.1/168.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m78.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n  Attempting uninstall: torch\n    Found existing installation: torch 2.4.0\n    Uninstalling torch-2.4.0:\n      Successfully uninstalled torch-2.4.0\nSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 torch-2.3.0 triton-2.3.0\nCollecting torchvision==0.18.0\n  Downloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (6.6 kB)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0) (1.26.4)\nRequirement already satisfied: torch==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0) (2.3.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision==0.18.0) (10.3.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch==2.3.0->torchvision==0.18.0) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch==2.3.0->torchvision==0.18.0) (12.6.77)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch==2.3.0->torchvision==0.18.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch==2.3.0->torchvision==0.18.0) (1.3.0)\nDownloading torchvision-0.18.0-cp310-cp310-manylinux1_x86_64.whl (7.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m54.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchvision\n  Attempting uninstall: torchvision\n    Found existing installation: torchvision 0.19.0\n    Uninstalling torchvision-0.19.0:\n      Successfully uninstalled torchvision-0.19.0\nSuccessfully installed torchvision-0.18.0\nCollecting torchtext==0.18.0\n  Downloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl.metadata (7.9 kB)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torchtext==0.18.0) (4.66.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torchtext==0.18.0) (2.32.3)\nRequirement already satisfied: torch>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from torchtext==0.18.0) (2.3.0)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torchtext==0.18.0) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.15.1)\nRequirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (4.12.2)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (1.13.3)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.3)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (3.1.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (2024.6.1)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\nRequirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (8.9.2.26)\nRequirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.3.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (11.0.2.54)\nRequirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (10.3.2.106)\nRequirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (11.4.5.107)\nRequirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.0.106)\nRequirement already satisfied: nvidia-nccl-cu12==2.20.5 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (2.20.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (12.1.105)\nRequirement already satisfied: triton==2.3.0 in /opt/conda/lib/python3.10/site-packages (from torch>=2.3.0->torchtext==0.18.0) (2.3.0)\nRequirement already satisfied: nvidia-nvjitlink-cu12 in /opt/conda/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=2.3.0->torchtext==0.18.0) (12.6.77)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.18.0) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.18.0) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.18.0) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torchtext==0.18.0) (2024.8.30)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=2.3.0->torchtext==0.18.0) (2.1.5)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=2.3.0->torchtext==0.18.0) (1.3.0)\nDownloading torchtext-0.18.0-cp310-cp310-manylinux1_x86_64.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hInstalling collected packages: torchtext\nSuccessfully installed torchtext-0.18.0\nRequirement already satisfied: kagglehub in /opt/conda/lib/python3.10/site-packages (0.3.1)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from kagglehub) (21.3)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from kagglehub) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from kagglehub) (4.66.4)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->kagglehub) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->kagglehub) (2024.8.30)\n","output_type":"stream"}]},{"cell_type":"code","source":"import kagglehub\nimport pandas as pd\npath = kagglehub.dataset_download(\"jessicali9530/celeba-dataset\")","metadata":{"id":"Rrop5z1KLsOB","execution":{"iopub.status.busy":"2024-11-16T17:57:02.006807Z","iopub.execute_input":"2024-11-16T17:57:02.007228Z","iopub.status.idle":"2024-11-16T17:57:03.312317Z","shell.execute_reply.started":"2024-11-16T17:57:02.007182Z","shell.execute_reply":"2024-11-16T17:57:03.311259Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"from torchvision.io import read_image\nimport torchvision.transforms as transforms\nfrom torchvision.transforms import ToTensor\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport os\n\nopposites = {\n  '5_o_Clock_Shadow': 'Clean_Shaven',\n  'Arched_Eyebrows': 'Flat_Eyebrows',\n  'Attractive': 'Unattractive',\n  'Bags_Under_Eyes': 'Clean_Eyes',\n  'Bald': 'Has_Hair',\n  'Bangs': 'No_Bangs',\n  'Big_Lips': 'Thin_Lips',\n  'Big_Nose': 'Small_Nose',\n  'Black_Hair': 'Bright_Hair',\n  'Blond_Hair': 'Non_Blond_Hair',\n  'Blurry': 'Clear',\n  'Brown_Hair': 'Non_Brown_Hair',\n  'Bushy_Eyebrows': 'Thin_Eyebrows',\n  'Chubby': 'Slim',\n  'Double_Chin': 'Single_Chin',\n  'Eyeglasses': 'No_Eyeglasses',\n  'Goatee': 'No_Goatee',\n  'Gray_Hair': 'Non_Gray_Hair',\n  'Heavy_Makeup': 'No_Makeup',\n  'High_Cheekbones': 'Low_Cheekbones',\n  'Male': 'Female',\n  'Mouth_Slightly_Open': 'Mouth_Closed',\n  'Mustache': 'No_Mustache',\n  'Narrow_Eyes': 'Wide_Eyes',\n  'No_Beard': 'Beard',\n  'Oval_Face': 'Round_Face',\n  'Pale_Skin': 'Tan_Skin',\n  'Pointy_Nose': 'Flat_Nose',\n  'Receding_Hairline': 'Full_Hairline',\n  'Rosy_Cheeks': 'Pale_Cheeks',\n  'Sideburns': 'No_Sideburns',\n  'Smiling': 'Not_Smiling',\n  'Straight_Hair': 'Curly_Hair',\n  'Wavy_Hair': 'Straight_Hair',\n  'Wearing_Earrings': 'No_Earrings',\n  'Wearing_Hat': 'No_Hat',\n  'Wearing_Lipstick': 'No_Lipstick',\n  'Wearing_Necklace': 'No_Necklace',\n  'Wearing_Necktie': 'No_Necktie',\n  'Young': 'Old'\n}\n\nclass CustomDataset(Dataset):\n    def __init__(self, annotations_file, img_dir, opposites,image_transform):\n        self.img_labels = pd.read_csv(annotations_file)\n        self.img_dir = img_dir\n        self.opposites=opposites\n        self.image_transform=image_transform\n\n    def __len__(self):\n        return len(self.img_labels)\n\n    def __getitem__(self, image_num):\n        # return one face and corresponding description\n        img_path = os.path.join(self.img_dir, self.img_labels.iloc[image_num, 0])\n        image = read_image(img_path)\n        image_tensor=self.image_transform(image)\n\n        description_series=self.img_labels.iloc[image_num]\n        columns=self.img_labels.columns\n        description=[column_name if description_series[column_name]==1 else self.opposites[column_name] for column_name in columns[1:]]\n        description=' '.join(description)\n\n        return image_tensor, description\n\nimage_transform = transforms.Compose([\n    transforms.ConvertImageDtype(torch.float),  # Convert to float in range [0, 1]\n    # transforms.CenterCrop((128, 128)),\n    transforms.Resize((256,256)),#Pictures will be not natural proportions (178*216)->(256*256)\n    transforms.Normalize([0.5], [0.5])\n])\ndataset=CustomDataset(path+'/list_attr_celeba.csv',path+'/img_align_celeba/img_align_celeba',opposites,image_transform)\ndataloader=DataLoader(dataset,batch_size=32,shuffle=True)","metadata":{"id":"F6NKlSPn1KiR","execution":{"iopub.status.busy":"2024-11-16T17:57:03.314207Z","iopub.execute_input":"2024-11-16T17:57:03.314784Z","iopub.status.idle":"2024-11-16T17:57:07.786347Z","shell.execute_reply.started":"2024-11-16T17:57:03.314736Z","shell.execute_reply":"2024-11-16T17:57:07.785346Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch import nn\nfrom torch.nn import functional as F\nimport math\nimport numpy as np","metadata":{"id":"jxQT5_eaJ8Am","execution":{"iopub.status.busy":"2024-11-16T17:57:07.788452Z","iopub.execute_input":"2024-11-16T17:57:07.788798Z","iopub.status.idle":"2024-11-16T17:57:07.793216Z","shell.execute_reply.started":"2024-11-16T17:57:07.788765Z","shell.execute_reply":"2024-11-16T17:57:07.792214Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"# Noising image","metadata":{"id":"cF0ARiDsID-g"}},{"cell_type":"code","source":"class Scheduler:\n    def __init__(self, num_steps=1000, step_size=1,beta_start=0.0001, beta_end=0.008, img_size=128):\n        # linear schedule\n        self.betas=torch.linspace(beta_start,beta_end,num_steps,dtype=torch.float32)\n        self.alphas=1.0-self.betas\n        self.alphas_cumprod=torch.cumprod(self.alphas,0)\n\n        self.img_size=img_size\n        self.amount_steps=num_steps\n        self.step_size=step_size # =20 on inference mode\n        self.steps_tensor = torch.from_numpy(np.arange(0, num_steps)[::-1].copy())\n        self.current_step=num_steps-1\n    \n    def add_noise(self, original_images, timestep):\n        # (4) cumulative values for fast calculation of timestep image batch with noise\n        # vector form calculations\n        # images are in latent representations\n        sqrt_alphas_cumprod=self.alphas_cumprod[timestep]\n        sqrt_alphas_cumprod=sqrt_alphas_cumprod**0.5\n        current_alpha_cumprod=self.alphas_cumprod[timestep]\n        one_minus_alphas_cumprod=(1.0-current_alpha_cumprod)\n        # Sample noise\n        noise=torch.randn(original_images.shape, dtype=original_images.dtype, device=original_images.device)\n        # return weighted sum of images\n        noised_images=original_images*sqrt_alphas_cumprod+one_minus_alphas_cumprod*noise\n        return noised_images\n\n    def denoising_step(self, timestep_current, noised_image_latent_current, noise_latent_current):\n        # predict previous image\n        # noise_latent_current is obtained from unet\n\n        # previous timestep. On inference step_size is bigger in sake of speed\n        timestep_previous=timestep_current-self.amount_steps//self.step_size\n\n        # get precomputed cumulative alpha - coefficients of original image\n        alpha_cumprod_current=self.alphas_cumprod[timestep_current]\n        alpha_cumprod_previous=self.alphas_cumprod[timestep_previous] if timestep_previous >=0 else torch.tensor(1.0)\n        # get cumulative betas - coefficients of noise\n        beta_cumprod_current=1.-alpha_cumprod_current\n        beta_cumprod_previous=1.-alpha_cumprod_previous\n        # get current_timestamp's alpha and beta from cumulative values division\n        alpha_current=alpha_cumprod_current/alpha_cumprod_previous\n        beta_current=1-alpha_current\n\n        # estimate means we calculate it using orignal+noise*coef?\n        # compute original_sample_latent_estimate as image_latent_current - noise_latent_current\n        predicted_original_image_latent=(noised_image_latent_current - (beta_cumprod_current**0.5)*noise_latent_current)/(alpha_cumprod_current**0.5)\n        # compute weights for predicted_original and noise\n        coef_to_original_to_make_previous_current=((alpha_cumprod_previous**0.5)*beta_current)/beta_cumprod_current\n        coef_to_make_previous_from_current=(alpha_current**0.5)*beta_cumprod_previous/beta_cumprod_current\n\n        # In other words, Previous without noise = current*0.99 + original*0.01\n        predicted_previous_image_latent=coef_to_make_previous_from_current*noised_image_latent_current + coef_to_original_to_make_previous_current*predicted_original_image_latent\n\n        # Since process of noising is stochastic, we dont know exact image at each step of noising\n        # Capture uncertainty with variance\n        if timestep_current==0:\n            uncertainty=0\n            coef_uncertainty=0\n        elif timestep_current>0:\n            uncertainty=torch.randn(noise_latent_current.shape,dtype=noise_latent_current.dtype,device=noise_latent_current.device)\n            # The closer to timestep=0, the less coef_uncertainty\n            coef_uncertainty=(1-alpha_cumprod_previous)/(1-alpha_cumprod_current)*beta_current\n            coef_uncertainty=torch.clamp(coef_uncertainty,min=1e-20)\n\n        predicted_previous_image_latent=predicted_previous_image_latent+coef_uncertainty*uncertainty\n        return predicted_previous_image_latent\n\n    def set_inference_mode(self, step_size=20):\n        self.step_size=20\n        \n    def reset_parameters(self,step_size=1):\n        self.step_size=step_size\n        self.current_step=self.amount_steps-1","metadata":{"id":"BVOzSc9SIHC-","execution":{"iopub.status.busy":"2024-11-16T17:57:07.794855Z","iopub.execute_input":"2024-11-16T17:57:07.795223Z","iopub.status.idle":"2024-11-16T17:57:07.848027Z","shell.execute_reply.started":"2024-11-16T17:57:07.795177Z","shell.execute_reply":"2024-11-16T17:57:07.846965Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n# ddpmsampler=Scheduler()\n\n# for images_batch,descriptions_batch in dataloader:\n#   print(images_batch.shape)\n#   print(descriptions_batch)\n#   noised_images_batch=ddpmsampler.add_noise(images_batch,399)\n#   batch_size = noised_images_batch.size(0)\n#   fig, axes = plt.subplots(1, batch_size, figsize=(15, 5))\n\n#   for i, image in enumerate(noised_images_batch):\n#       axes[i].imshow(image.permute(1, 2, 0).cpu().numpy())\n#       axes[i].axis('off')\n\n#   plt.show()\n#   break\n  # TODO: fix clipping for better output","metadata":{"id":"g75cFBfEV1nF","execution":{"iopub.status.busy":"2024-11-16T17:57:07.849339Z","iopub.execute_input":"2024-11-16T17:57:07.850002Z","iopub.status.idle":"2024-11-16T17:57:07.892791Z","shell.execute_reply.started":"2024-11-16T17:57:07.849924Z","shell.execute_reply":"2024-11-16T17:57:07.891599Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"# Variational Autoencoder\n\nFirstly, high resolution noise is encoded into lower (latent) dimenstion in order to make matrices smaller and have less compute. Thus, we need Encoder part and Decoder part","metadata":{"id":"gMHYBgwLIstz"}},{"cell_type":"code","source":"class MultiheadAttention(nn.Module):\n    def __init__(self, amount_heads, dim_q, dim_kv=None,qkv_bias=True,o_bias=True):\n        super().__init__()\n        # Multihead to capture different relationships\n        self.amount_heads=amount_heads\n        self.head_dim=dim_q//amount_heads\n\n        # Learnable projection of embedding to highlight usage in particular purpose\n        self.Q_matrix=nn.Linear(dim_q,dim_q,bias=qkv_bias)\n        if dim_kv==None:\n            self.K_matrix=nn.Linear(dim_q,dim_q,bias=qkv_bias)\n            self.V_matrix=nn.Linear(dim_q,dim_q,bias=qkv_bias)\n        else:\n            self.K_matrix=nn.Linear(dim_kv,dim_q,bias=qkv_bias)\n            self.V_matrix=nn.Linear(dim_kv,dim_q,bias=qkv_bias)\n        # Project back to initial embedding space\n        self.O_matrix=nn.Linear(dim_q,dim_q,bias=o_bias)\n\n    def forward(self, query, key=None, value=None, selfattention_mask=None):\n        # Expect query to be Batch,seq_len=channel,embed_dim=H/8*W/8\n        input_shape=query.shape\n        batch_size, sequence_length, d_embed = query.shape\n        if key==None and value==None:\n            #self-attention\n            key=query\n            value=query\n            #?\n            interim_shape = (batch_size, sequence_length, self.amount_heads, self.head_dim)\n        else:\n            #?\n            interim_shape = (batch_size, -1, self.amount_heads, self.head_dim)\n\n        # Projections\n        q_proj=self.Q_matrix(query).view(interim_shape).transpose(1, 2)\n        k_proj=self.K_matrix(key).view(interim_shape).transpose(1, 2)\n        v_proj=self.V_matrix(value).view(interim_shape).transpose(1, 2)\n\n        attention_logits = (q_proj @ k_proj.transpose(-1, -2)) # Logits #-2,-1?\n        if selfattention_mask is not None:\n            # Mask where the upper triangle (above the principal diagonal) is 1\n            mask = torch.ones_like(attention_logits, dtype=torch.bool).triu(1)\n            # Fill the upper triangle with -inf\n            attention_logits = attention_logits.masked_fill(mask, -torch.inf)\n\n        # Normalize values. When having long sequence, attention value will naturally grow\n        attention_logits=attention_logits/math.sqrt(self.head_dim)\n        attention_weights = F.softmax(attention_logits, dim=-1)\n\n        context_aware_x=attention_weights @ v_proj\n        context_aware_x = context_aware_x.transpose(1, 2).contiguous().view(input_shape)\n        context_aware_x=self.O_matrix(context_aware_x)\n        return context_aware_x","metadata":{"id":"PFUsX_0UXACs","execution":{"iopub.status.busy":"2024-11-16T17:57:07.894270Z","iopub.execute_input":"2024-11-16T17:57:07.894998Z","iopub.status.idle":"2024-11-16T17:57:07.918518Z","shell.execute_reply.started":"2024-11-16T17:57:07.894947Z","shell.execute_reply":"2024-11-16T17:57:07.917683Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class VAEMultiheadAttention(nn.Module):\n    def __init__(self,channels):\n        super().__init__()\n        # Used, since convolution output features are related in groups\n        # E.g. Dark eyes area is differ from bright nose area\n        # Thus features representing should be normalized in local groups\n        self.groupnorm=nn.GroupNorm(16,channels)\n        self.multiheadattention=MultiheadAttention(1,channels) #TODO: check self/cross\n\n    def forward(self,x):\n        # keep initial information\n        residue=x\n        x=self.groupnorm(x)\n\n        # print(x.shape)\n        batch,channel,height,width=x.shape\n        x=x.view(batch,channel,height*width)\n        x = x.transpose(-1, -2) #?\n\n        x=self.multiheadattention(x)\n\n        x = x.transpose(-1, -2)\n        x=x.view(batch,channel,height,width)\n\n        x=x+residue\n        return x\n\nclass VAEResidualBlock(nn.Module):\n    def __init__(self,in_channels,out_channels):\n        super().__init__()\n        self.groupnorm_1 = nn.GroupNorm(16, in_channels)\n        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n\n        self.groupnorm_2 = nn.GroupNorm(16, out_channels)\n        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n\n        if in_channels == out_channels:\n            self.residual_layer = nn.Identity()\n        else:\n            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n\n    def forward(self,x):\n        # match residue to output\n        residue=self.residual_layer(x)\n\n        x=self.groupnorm_1(x)\n        x=F.silu(x)\n        x=self.conv_1(x)\n        x=self.groupnorm_2(x)\n        x=F.silu(x)\n        x=self.conv_2(x)\n\n        return x+residue","metadata":{"id":"R8Rra20IIXl8","execution":{"iopub.status.busy":"2024-11-16T17:57:07.919769Z","iopub.execute_input":"2024-11-16T17:57:07.920154Z","iopub.status.idle":"2024-11-16T17:57:07.933632Z","shell.execute_reply.started":"2024-11-16T17:57:07.920114Z","shell.execute_reply":"2024-11-16T17:57:07.932857Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn\nclass VAEEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            # B,3,H,W -> B,32,H,W\n            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n            VAEResidualBlock(32, 32),\n            VAEResidualBlock(32, 32),\n            # B,32,H,W->B,32,H/2,W/2\n            nn.Conv2d(32, 32, kernel_size=3, stride=2, padding=1),\n            VAEResidualBlock(32, 64),\n            VAEResidualBlock(64, 64),\n            # B,64,H/2,W/2->B,64,H/4,W/4\n            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=1),\n            VAEResidualBlock(64, 128),\n            VAEResidualBlock(128, 128),\n            ## B,128,H/4,W/4->B,128,H/8,W/8\n            nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            #B,128=channel,H/8,W/8->B,128,H/8,W/8\n            VAEMultiheadAttention(128),\n            VAEResidualBlock(128, 128),\n            nn.GroupNorm(16, 128),\n            nn.SiLU(),\n            nn.Conv2d(128, 8, kernel_size=3, padding=1),\n            nn.Conv2d(8, 8, kernel_size=1, padding=0) #8,8,1,1\n        ])\n\n    def forward(self, x):\n        for module in self.layers:\n            x = module(x) #?\n        mean, log_variance = torch.chunk(x, 2, dim=1)\n        log_variance = torch.clamp(log_variance, -30, 20)\n        variance = log_variance.exp()\n        stdev = variance.sqrt()\n        noise = torch.randn(stdev.shape, dtype=stdev.dtype, device=stdev.device)\n        x = mean + stdev * noise\n        x = x * 0.18215\n        return x\n\n\nclass VAEDecoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            nn.Conv2d(4, 4, kernel_size=1, padding=0),\n            nn.Conv2d(4, 128, kernel_size=3, padding=1),\n            VAEResidualBlock(128, 128),\n            VAEMultiheadAttention(128),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            VAEResidualBlock(128, 128),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            VAEResidualBlock(128, 64),\n            VAEResidualBlock(64, 64),\n            VAEResidualBlock(64, 64),\n            nn.Upsample(scale_factor=2),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            VAEResidualBlock(64, 32),\n            VAEResidualBlock(32, 32),\n            VAEResidualBlock(32, 32),\n            nn.GroupNorm(4, 32),\n            nn.SiLU(),\n            nn.Conv2d(32, 3, kernel_size=3, padding=1)\n        ])\n\n    def forward(self, x):\n        for module in self.layers:\n            x = module(x)\n        x = x / 0.18125\n        return x\n\n#TODO: channels*=2","metadata":{"id":"QdLSSOfVo6PX","execution":{"iopub.status.busy":"2024-11-16T17:57:07.934765Z","iopub.execute_input":"2024-11-16T17:57:07.935038Z","iopub.status.idle":"2024-11-16T17:57:07.952373Z","shell.execute_reply.started":"2024-11-16T17:57:07.935009Z","shell.execute_reply":"2024-11-16T17:57:07.951616Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# encoder=VAEEncoder()\n# decoder=VAEDecoder()\n\n# for images_batch,descriptions_batch in dataloader:\n#   encoded_images=encoder(images_batch)\n#   decoded_images=decoder(encoded_images)\n#   print('decoded shape',decoded_images.shape)\n\n#   batch_size = encoded_images.size(0)\n#   fig, axes = plt.subplots(1, batch_size, figsize=(15, 5))\n\n#   for i, image in enumerate(encoded_images):\n#       axes[i].imshow(image.permute(1, 2, 0).cpu().detach().numpy())\n#       axes[i].axis('off')\n#   break","metadata":{"id":"Ki7J2GLqjzTw","execution":{"iopub.status.busy":"2024-11-16T17:57:07.955530Z","iopub.execute_input":"2024-11-16T17:57:07.955833Z","iopub.status.idle":"2024-11-16T17:57:07.963814Z","shell.execute_reply.started":"2024-11-16T17:57:07.955804Z","shell.execute_reply":"2024-11-16T17:57:07.963046Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Text Encoder\n\nAt each step in the reverse diffusion, CLIP can compare the current generated image's embedding with the target text embedding and adjust the denoising direction accordingly. To be able to compare input prompt with denoised image at current timestep, clip should learn projections of initial image/text embedding to map to same space.\n\nAfter similarity score is recieved, subtract it from loss: MSELoss(real_noise,predicted_unet_loss)-weight*similarity\n\nTrade off vanilla smooth loss landscape to new model feature.","metadata":{"id":"xgPY-fgu5rbw"}},{"cell_type":"code","source":"# from transformers import AutoModel, AutoTokenizer, BertTokenizer\n# from torchvision import models\n# from dataclasses import dataclass\n\n# dataclass\n# class Config:\n#     embed_dim: int = 512\n#     transformer_embed_dim: int = 768\n#     max_len: int = 32\n#     text_model: str = \"distilbert-base-multilingual-cased\"\n#     epochs: int = 5\n#     batch_size: int = 128\n\n# class Projection(nn.Module):\n#     # Project initial embedding into common space\n#     def __init__(self, d_in: int, d_out: int, p: float=0.5) -> None:\n#         super().__init__()\n#         self.linear1 = nn.Linear(d_in, d_out, bias=False)\n#         self.linear2 = nn.Linear(d_out, d_out, bias=False)\n#         self.layer_norm = nn.LayerNorm(d_out)\n#         self.drop = nn.Dropout(p)\n\n#     def forward(self, x: torch.Tensor) -> torch.Tensor:\n#         embed1 = self.linear1(x)\n#         embed2 = self.drop(self.linear2(F.gelu(embed1))) # non-linearity and small gradient fix\n#         embeds = self.layer_norm(embed1 + embed2)\n#         return embeds\n\n# class VisionEncoder(nn.Module):\n#     def __init__(self, d_out: int) -> None:\n#         super().__init__()\n#         self.base = models.resnet34(pretrained=True)\n#         d_in = self.base.fc.in_features\n#         self.base.fc = nn.Identity() #classification layer=I, resnet should return pure embeddings\n#         self.projection = Projection(d_in, d_out)\n#         for p in self.base.parameters():\n#             p.requires_grad = False\n\n#     def forward(self, x):\n#         projected_vec = self.projection(self.base(x))\n#         projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n#         return projected_vec / projection_len # normalize to len==1\n\n# class TextEncoder(nn.Module):\n#     def __init__(self, d_out: int) -> None:\n#       super().__init__()\n#       self.base = AutoModel.from_pretrained(Config.text_model)\n#       self.projection = Projection(Config.transformer_embed_dim, d_out)\n#       for p in self.base.parameters():\n#           p.requires_grad = False\n\n#     def forward(self, x):\n#       #expects {input_ids:,attention_mask:} input from tokenizer\n#       out = self.base(x)[0]#? [0]\n#       out = out[:, 0, :]  # get CLS token output = embedding #?[:,0,:]\n#       projected_vec = self.projection(out)\n#       projection_len = torch.norm(projected_vec, dim=-1, keepdim=True)\n#       return projected_vec / projection_len\n\n# def CLIP_loss(logits: torch.Tensor) -> torch.Tensor:\n#     n = logits.shape[1]      # number of samples\n#     labels = torch.arange(n) # Create labels tensor\n#     #  image-to-caption similarity\n#     loss_i = F.cross_entropy(logits.transpose(0, 1), labels, reduction=\"mean\")\n#     # caption-to-image similarity\n#     loss_t = F.cross_entropy(logits, labels, reduction=\"mean\")\n#     # Calculate the final loss\n#     loss = (loss_i + loss_t) / 2\n\n#     return loss\n\n# def metrics(similarity: torch.Tensor):\n#     y = torch.arange(len(similarity)).to(similarity.device)\n#     img2cap_match_idx = similarity.argmax(dim=1)\n#     cap2img_match_idx = similarity.argmax(dim=0)\n\n#     img_acc = (img2cap_match_idx == y).float().mean()\n#     cap_acc = (cap2img_match_idx == y).float().mean()\n\n#     return img_acc, cap_acc\n\n# class Tokenizer:\n#     def __init__(self, tokenizer: BertTokenizer) -> None:\n#         self.tokenizer = tokenizer\n\n#     def __call__(self, x: str) -> AutoTokenizer:\n#         return self.tokenizer(\n#             x,\n#             max_length=Config.max_len,\n#             truncation=True,\n#             padding=True,\n#             return_tensors=\"pt\",\n#         )\n\n# class CLIP(nn.Module):\n#   def __init__(self, lr: float = 1e-3) -> None:\n#     super().__init__()\n#     self.vision_encoder = VisionEncoder(Config.embed_dim)\n#     self.caption_encoder = TextEncoder(Config.embed_dim)\n#     self.tokenizer = Tokenizer(AutoTokenizer.from_pretrained(Config.text_model))\n#     self.lr = lr\n#     self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n#   def forward(self, images, text):\n#     # tensor image, tuple of texts as input\n#     text_tokenized = self.tokenizer(text).to(self.device)\n#     caption_embed = self.caption_encoder(text_tokenized[\"input_ids\"])\n#     print(caption_embed.shape)\n#     image_embed = self.vision_encoder(images)\n#     print(image_embed.shape)\n#     similarity = caption_embed.T @ image_embed\n\n#     return similarity\n\n# clip=CLIP()\n\n# optimizer = torch.optim.Adam(\n#     [\n#         {\"params\": clip.vision_encoder.parameters()},\n#         {\"params\": clip.caption_encoder.parameters()},\n#     ],\n#     lr=clip.lr,\n# )\n\n# for epoch in range(1,2):\n#     clip.train()\n#     for batch in dataloader:\n#         image, text = batch\n#         print(len(text),text)\n#         similarity = clip(image, text)\n#         print(similarity.shape)\n#         loss = CLIP_loss(similarity)\n#         img_acc, cap_acc = metrics(similarity)\n\n#         optimizer.zero_grad()\n#         loss.backward()\n#         optimizer.step()\n#         break\n\n#     print(f\"Epoch [epoch+1/num_epochs], Batch Loss: {loss.item()}, img_acc,cap_acc: {img_acc,cap_acc}\")","metadata":{"id":"7BVBOigd5wJy","execution":{"iopub.status.busy":"2024-11-16T17:57:07.964954Z","iopub.execute_input":"2024-11-16T17:57:07.965242Z","iopub.status.idle":"2024-11-16T17:57:07.974808Z","shell.execute_reply.started":"2024-11-16T17:57:07.965195Z","shell.execute_reply":"2024-11-16T17:57:07.973832Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\nfrom collections import Counter\n\nclass TextEmbedding(nn.Module):\n    def __init__(self, n_vocab: int, n_embd: int, n_token: int):\n        super().__init__()\n\n        self.token_embedding = nn.Embedding(n_vocab, n_embd)\n        # A learnable weight matrix encodes the position information for each token\n        self.position_embedding = nn.Parameter(torch.zeros((n_token, n_embd)))\n\n    def forward(self, tokens):\n        x = self.token_embedding(tokens)\n        x = x + self.position_embedding\n        return x\n\nclass EncoderLayer(nn.Module):\n    def __init__(self, n_head: int, n_embd: int):\n        super().__init__()\n        self.layernorm_1 = nn.LayerNorm(n_embd)\n        self.attention = MultiheadAttention(n_head, n_embd)\n        self.layernorm_2 = nn.LayerNorm(n_embd)\n        self.linear_1 = nn.Linear(n_embd, 4 * n_embd)\n        self.linear_2 = nn.Linear(4 * n_embd, n_embd)\n\n    def forward(self, x):\n        # (Batch_Size, Seq_Len, Dim)\n        residue = x\n\n        # Learning representations\n        x = self.layernorm_1(x)\n        x = self.attention(x, selfattention_mask=True)\n        x = x+ residue\n\n        # Thinking about hierarchical relationships\n        residue = x\n        x = self.layernorm_2(x)\n        x = self.linear_1(x)\n        x = x * torch.sigmoid(1.702 * x)   # QuickGELU activation function\n        x = self.linear_2(x)\n        x =x+residue\n\n        return x\n\nclass TextEncoder(nn.Module):\n    # Text embedding\n    def __init__(self):\n        super().__init__()\n        self.embedding = TextEmbedding(81, 32, 40)\n\n        self.layers = nn.ModuleList([\n            EncoderLayer(8, 32) for i in range(8)\n        ])\n\n        self.layernorm = nn.LayerNorm(32)\n\n    def forward(self, tokens: torch.LongTensor) -> torch.FloatTensor:\n        # Expect tokenized input\n        tokens = tokens.type(torch.long)\n        state = self.embedding(tokens)\n        for layer in self.layers:\n            state = layer(state)\n        output = self.layernorm(state)\n        return output\n\nclass Tokenizer:\n    def __init__(self, opposites):\n        all_words = set()\n        for key, value in opposites.items():\n            all_words.add(key)\n            all_words.add(value)\n        self.build_vocab(all_words)\n\n    def build_vocab(self, data):\n        tokenized_data = [[word] for word in data]  # Each word in its own list\n        self.vocab = build_vocab_from_iterator(tokenized_data, specials=[\"<pad>\", \"<unk>\"])\n        self.vocab.set_default_index(self.vocab[\"<unk>\"])  # Handle unknown tokens\n\n    def tokenize(self, batch_text_string):\n        batch = []\n        for text in batch_text_string:\n            text_splitted = text.split()  # Split on whitespace to get words\n            input_ids = [self.vocab[token] for token in text_splitted]\n            batch.append(input_ids)\n        return batch","metadata":{"id":"zvB88qIF_P3L","colab":{"base_uri":"https://localhost:8080/"},"outputId":"ce7c9c35-e3bb-4106-a916-c3c06d3c411c","execution":{"iopub.status.busy":"2024-11-16T17:57:07.975996Z","iopub.execute_input":"2024-11-16T17:57:07.976246Z","iopub.status.idle":"2024-11-16T17:57:08.007989Z","shell.execute_reply.started":"2024-11-16T17:57:07.976218Z","shell.execute_reply":"2024-11-16T17:57:08.007210Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n/opt/conda/lib/python3.10/site-packages/torchtext/utils.py:4: UserWarning: \n/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \nTorchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Only 40 words in dictionary, build custom tokenizer and learn custom embeddings\n\n# tokenizer = Tokenizer(opposites)\n# textencoder = TextEncoder()\n\n# for images_batch, descriptions_batch in dataloader:\n#     input_ids = tokenizer.tokenize(descriptions_batch)\n#     input_ids = torch.tensor(input_ids, dtype=torch.long)\n#     embeddings = textencoder(input_ids)\n#     print(embeddings)\n#     break","metadata":{"id":"3KDl7ZM5bxMX","execution":{"iopub.status.busy":"2024-11-16T17:57:08.008875Z","iopub.execute_input":"2024-11-16T17:57:08.009120Z","iopub.status.idle":"2024-11-16T17:57:08.013279Z","shell.execute_reply.started":"2024-11-16T17:57:08.009093Z","shell.execute_reply":"2024-11-16T17:57:08.012441Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Unet","metadata":{"id":"qY5aiWnTVERO"}},{"cell_type":"code","source":"class UNETResidualConvolution(nn.Module):\n    # keep track of current timestep via adjusting image with timestep_embedded\n    def __init__(self,in_channels,out_channels, timestamp_dim=1280):\n        super().__init__()\n        # split vector into 32 groups and normalize separetly\n        self.groupnorm_1 = nn.GroupNorm(32, in_channels)\n        self.conv_1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n        # transform initial timestamp embedding into out_channels\n        self.linear1 = nn.Linear(timestamp_dim, out_channels)\n\n        self.groupnorm_2 = nn.GroupNorm(32, out_channels)\n        self.conv_2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n\n        if in_channels == out_channels:\n            self.residual_layer = nn.Identity()\n        else:\n            self.residual_layer = nn.Conv2d(in_channels, out_channels, kernel_size=1, padding=0)\n\n    def forward(self,x, timestamp_embedded):\n        # match residue to output\n        residue=self.residual_layer(x)\n\n        timestamp_embedded=F.silu(timestamp_embedded)\n        timestamp_embedded=self.linear1(timestamp_embedded)\n\n        x=self.groupnorm_1(x)\n        x=F.silu(x)\n        x=self.conv_1(x)\n\n        # Add time to width and height matrix\n        # (Batch_Size, Out_Channels, Height, Width) + (1, Out_Channels, 1, 1) -> (Batch_Size, Out_Channels, Height, Width)\n        merged=x+timestamp_embedded.unsqueeze(-1).unsqueeze(-1)\n\n        merged=self.groupnorm_2(merged)\n        merged=F.silu(x)\n        merged=self.conv_2(merged)\n\n        return merged+residue\n\nclass UNETAttention(nn.Module):\n\n    def __init__(self,n_head,head_dim,context_dim=32):\n        super().__init__()\n        channels = n_head * head_dim\n        self.channels=channels\n\n        self.groupnorm = nn.GroupNorm(32, channels, eps=1e-6)\n        self.conv_input = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n\n        self.layernorm_1 = nn.LayerNorm(channels)\n        # selfattention within image\n        self.attention_1 = MultiheadAttention(n_head, channels, qkv_bias=False)\n\n        self.layernorm_2 = nn.LayerNorm(channels)\n        # Here, cross attention between image Query and text Key,Values\n        self.attention_2 = MultiheadAttention(n_head, channels, dim_kv=context_dim, qkv_bias=False)\n\n        self.layernorm_3 = nn.LayerNorm(channels)\n        self.linear_geglu_1  = nn.Linear(channels, 4 * channels * 2)\n        self.linear_geglu_2 = nn.Linear(4 * channels, channels)\n\n        self.conv_output = nn.Conv2d(channels, channels, kernel_size=1, padding=0)\n\n    def forward(self,image,text):\n        # image: [1, 320, 16, 16]\n        # text   [2, 40, 32]\n        # print(image.shape,text.shape)\n        residue_long=image\n        x=self.groupnorm(image)\n        x=self.conv_input(x)\n        n, c, h, w = x.shape\n        x = x.view((n, c, h * w))\n        x = x.transpose(-1, -2)\n\n        residue_short=x\n        x=self.layernorm_1(x)\n        # self attention for image\n        x=self.attention_1(x)\n        x=x+residue_short\n\n        residue_short=x\n        x=self.layernorm_2(x)\n        # cross attention in order to guide model\n        #[1, 256, 320]) torch.Size([2, 40, 32])\n        # print(x.shape,text.shape) #1,256->2,128\n        x=self.attention_2(x,text,text)\n        x=x+residue_short\n\n        residue_short=x\n        x=self.layernorm_3(x)\n        x,gate=self.linear_geglu_1(x).chunk(2,dim=-1)\n        x=x*F.gelu(gate)\n        x=self.linear_geglu_2(x)\n        x=x+residue_short\n\n        x = x.transpose(-1, -2)\n        x = x.view((n, c, h, w))\n\n        x=self.conv_output(x)\n        x=x+residue_long\n        return x\n\nclass Upsample(nn.Module):\n    # In Unet,starting from bottleneck Upsampling blocks are performed to get initial size of image_latent\n    #? Isnt latent represenation too small? use celeba256\n    def __init__(self, channels):\n        super().__init__()\n        self.conv = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n\n    def forward(self, x):\n        # (Batch_Size, Features, Height, Width) -> (Batch_Size, Features, Height * 2, Width * 2)\n        x = F.interpolate(x, scale_factor=2, mode='nearest')\n        x=self.conv(x)\n        return x\n\nclass SwitchSequential(nn.Sequential):\n    # Lazy model's modules run based on passed parameters\n    def forward(self, x, context, time):\n        for layer in self:\n            if isinstance(layer, UNETAttention):\n                x = layer(x, context)\n\n            elif isinstance(layer, UNETResidualConvolution):\n                x = layer(x, time)\n            else:\n                #conv2d or upsample\n                x = layer(x)\n        return x\n\nclass UNET(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n        self.encoders = nn.ModuleList([\n          SwitchSequential(nn.Conv2d(4, 320, kernel_size=3, padding=1)),\n\n#           SwitchSequential(UNETResidualConvolution(320, 320), UNETAttention(8, 40)),\n          SwitchSequential(UNETResidualConvolution(320, 320), UNETAttention(8, 40)),\n          # (Batch_Size, 320, Height / 8, Width / 8) is obtained from vae\n          # (Batch_Size, 320, Height / 8, Width / 8) -> (Batch_Size, 320, Height / 16, Width / 16)\n          SwitchSequential(nn.Conv2d(320, 320, kernel_size=3, stride=2, padding=1)),\n\n          SwitchSequential(UNETResidualConvolution(320, 640), UNETAttention(8, 80)),\n#           SwitchSequential(UNETResidualConvolution(640, 640), UNETAttention(8, 80)),\n\n          # (Batch_Size, 640, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 32, Width / 32)\n          SwitchSequential(nn.Conv2d(640, 640, kernel_size=3, stride=2, padding=1)),\n\n          SwitchSequential(UNETResidualConvolution(640, 1280), UNETAttention(8, 160)),\n#           SwitchSequential(UNETResidualConvolution(1280, 1280), UNETAttention(8, 160)),\n\n          # (Batch_Size, 1280, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 64, Width / 64)\n#           SwitchSequential(nn.Conv2d(1280, 1280, kernel_size=3, stride=2, padding=1)),\n\n#           SwitchSequential(UNETResidualConvolution(1280, 1280)),\n#           SwitchSequential(UNETResidualConvolution(1280, 1280)),\n      ])\n\n        self.bottleneck = nn.ModuleList([\n          SwitchSequential(UNETResidualConvolution(1280, 1280)),\n          SwitchSequential(UNETAttention(8, 160)),\n          SwitchSequential(UNETResidualConvolution(1280, 1280)),\n        ])\n\n        self.decoders = nn.ModuleList([\n#           SwitchSequential(UNETResidualConvolution(2560, 1280)),\n#           SwitchSequential(UNETResidualConvolution(2560, 1280)),\n\n          # (Batch_Size, 2560, Height / 64, Width / 64) -> (Batch_Size, 1280, Height / 32, Width / 32)\n#           SwitchSequential(UNETResidualConvolution(2560, 1280), Upsample(1280)),\n\n#           SwitchSequential(UNETResidualConvolution(2560, 1280), UNETAttention(8, 160)),\n          SwitchSequential(UNETResidualConvolution(2560, 1280), UNETAttention(8, 160)),\n\n          # (Batch_Size, 1920, Height / 32, Width / 32) -> (Batch_Size, 1280, Height / 16, Width / 16)\n          SwitchSequential(UNETResidualConvolution(1920, 1280), UNETAttention(8, 160), Upsample(1280)),\n\n          SwitchSequential(UNETResidualConvolution(1920, 640), UNETAttention(8, 80)),\n#           SwitchSequential(UNETResidualConvolution(1280, 640), UNETAttention(8, 80)),\n\n          # (Batch_Size, 960, Height / 16, Width / 16) -> (Batch_Size, 640, Height / 8, Width / 8)\n          SwitchSequential(UNETResidualConvolution(960, 640), UNETAttention(8, 80), Upsample(640)),\n          SwitchSequential(UNETResidualConvolution(960, 320), UNETAttention(8, 40)),\n\n#           SwitchSequential(UNETResidualConvolution(640, 320), UNETAttention(8, 40)),\n          SwitchSequential(UNETResidualConvolution(640, 320), UNETAttention(8, 40)),\n      ])\n\n    def forward(self,x,context,timestep_embedded):\n        # x: denoised latent picture (Batch_Size, 4, Height / 8, Width / 8)\n        # context: (Batch_Size, Seq_Len, Dim)\n        # time: (1, 1280)\n\n        skip_connections = []\n        for layer in self.encoders:\n            x = layer(x, context, timestep_embedded)\n            skip_connections.append(x)\n\n        for layer in self.bottleneck:\n            x = layer(x, context, timestep_embedded)\n\n#         print(len(skip_connections),len(self.decoders))\n        for layer in self.decoders:\n            corresponding_skip=skip_connections.pop()\n            x = torch.cat((x,corresponding_skip), dim=1)\n            x = layer(x, context, timestep_embedded)\n        \n        return x\n\nclass UNET_OutputLayer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.groupnorm = nn.GroupNorm(32, in_channels)\n        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n    def forward(self,x):\n        x=self.groupnorm(x)\n        x=F.silu(x)\n        x=self.conv(x)\n        return x\n\nclass TimeEmbedding(nn.Module):\n  # create complex time embedding\n    def __init__(self, timestamp_dim):\n        super().__init__()\n        self.linear1 = nn.Linear(timestamp_dim, 4 * timestamp_dim)\n        self.linear2 = nn.Linear(4 * timestamp_dim, 4 * timestamp_dim)\n\n    def forward(self, time):\n        # modifies one number into layer (1,320)\n        timestep_embedded=self.linear1(time)\n        timestep_embedded=F.silu(timestep_embedded)\n        timestep_embedded=self.linear2(timestep_embedded)\n        return timestep_embedded","metadata":{"id":"U2LmRxQxVHf8","execution":{"iopub.status.busy":"2024-11-16T17:57:08.014635Z","iopub.execute_input":"2024-11-16T17:57:08.014941Z","iopub.status.idle":"2024-11-16T17:57:08.051444Z","shell.execute_reply.started":"2024-11-16T17:57:08.014891Z","shell.execute_reply":"2024-11-16T17:57:08.050611Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"class Diffusion(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.time_embedding = TimeEmbedding(320)#? why 320\n        self.unet = UNET()\n        self.final = UNET_OutputLayer(320, 4)\n\n    def forward(self, latent, context, timestep):\n        # latent: (Batch_Size, 4, Height / 8, Width / 8)\n        # context: (Batch_Size, Seq_Len, Dim)\n        # time: (1, 320)\n\n        # (1, 320) -> (1, 1280)\n        timestep_embedded = self.time_embedding(timestep)\n\n        # (Batch, 4, Height / 8, Width / 8) -> (Batch, 320, Height / 8, Width / 8)\n        output = self.unet(latent, context, timestep_embedded)\n\n        # (Batch, 320, Height / 8, Width / 8) -> (Batch, 4, Height / 8, Width / 8)\n        output = self.final(output)\n\n#         print(\"Outside: input size\", latent.size(),\n#           \"output_size\", latent.size())\n        # (Batch, 4, Height / 8, Width / 8)\n        return output","metadata":{"id":"0KyH9dU2QkpM","execution":{"iopub.status.busy":"2024-11-16T17:57:08.052321Z","iopub.execute_input":"2024-11-16T17:57:08.052615Z","iopub.status.idle":"2024-11-16T17:57:08.064483Z","shell.execute_reply.started":"2024-11-16T17:57:08.052549Z","shell.execute_reply":"2024-11-16T17:57:08.063800Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"# Inference Pipeline","metadata":{"id":"7NUJuXscRklt"}},{"cell_type":"code","source":"def rescale(x, old_range, new_range, clamp=False):\n    old_min, old_max = old_range\n    new_min, new_max = new_range\n    x -= old_min\n    x *= (new_max - new_min) / (old_max - old_min)\n    x += new_min\n    if clamp:\n        x = x.clamp(new_min, new_max)\n    return x\n\ndef get_time_embedding(timestep):\n    # Shape: (160,): tensor([1.0000e+00, 9.4384e-01, 8.9021e-01, 8.3898e-01, ..., 1.0000e-04])\n    freqs = torch.pow(10000, -torch.arange(start=0, end=160, dtype=torch.float32) / 160)\n    # Shape: (1, 160): tensor([[10.0000,  9.4384,  8.9021,  8.3898, ..., 0.0010]])\n    x = torch.tensor([timestep], dtype=torch.float32)[:, None] * freqs[None]\n    # Shape: (1, 160 * 2): tensor([[cos(10.0000), cos(9.4384), ..., sin(10.0000), sin(9.4384), ...]])\n    return torch.cat([torch.cos(x), torch.sin(x)], dim=-1)\n\ndef image_context_similarity_loss(image, context):\n    batch_images_previous_prediction_resized = F.interpolate(\n        image, size=(32, 2048), mode=\"bilinear\", align_corners=False\n    )  # Shape: [Batch, 3, 32, 2048]\n\n    context_flattened = context.view(context.size(0), -1)  # Shape: [Batch, 1280]\n    context_flattened = context_flattened / context_flattened.norm(dim=-1, keepdim=True)\n    context_padded = F.pad(context_flattened, (0, 2048 - context_flattened.size(1)))  # Shape: [Batch, 2048]\n\n    context_padded_expanded = context_padded.view(context_padded.size(0), 1, 1, -1)  # Shape: [Batch, 1, 1, 2048]\n\n    # Compute dot product along the feature dimension (2048)\n    dot_product = torch.sum(\n        batch_images_previous_prediction_resized * context_padded_expanded, dim=-1\n    )  # Shape: [Batch, 3, 32]\n\n    # Reduce dot product to a scalar loss to be able to call .backward\n    similarity_loss = 1 - torch.mean(dot_product)\n    return similarity_loss","metadata":{"id":"Fny_bGGyCJlN","execution":{"iopub.status.busy":"2024-11-16T17:57:08.065578Z","iopub.execute_input":"2024-11-16T17:57:08.065879Z","iopub.status.idle":"2024-11-16T17:57:08.076806Z","shell.execute_reply.started":"2024-11-16T17:57:08.065849Z","shell.execute_reply":"2024-11-16T17:57:08.075866Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\ndef evaluate(diffusion,tokenizer,text_encoder,image_encoder,image_decoder,n_inference_steps,descriptions_batch,scheduler,do_cfg):\n    diffusion.eval()\n    text_encoder.eval()\n    image_encoder.eval()\n    image_decoder.eval()\n    \n    with torch.no_grad():\n        cond_tokens = tokenizer.tokenize(descriptions_batch)\n        cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n        cond_context = text_encoder(cond_tokens)\n        if do_cfg:\n            uncond_tokens = tokenizer.tokenize(descriptions_batch)\n            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n            uncond_context = text_encoder(uncond_tokens)\n            context = torch.cat([cond_context, uncond_context]) # B*2, 40, 32\n        else:\n            context = cond_context # B, 40, 32\n            \n        # set step_size\n        scheduler.set_inference_mode(scheduler.amount_steps//n_inference_steps)\n\n        # # Create noised latent image\n        latents_shape = (context.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH) # B, 4, 32, 32 #? what is 4?\n        latents = torch.randn(latents_shape, device=device)\n        \n        # move to gpu0\n        context=context.to(device)\n        latents=latents.to(device)\n        \n        timesteps = tqdm(range(scheduler.amount_steps-1,0,-scheduler.step_size))\n        for i, timestep in enumerate(timesteps):\n            scheduler.current_step=timestep\n            # for each denoising step, create timestep embedding from cosine/sine waves\n            time_embedding = get_time_embedding(timestep).to(device)\n            predicted_latent_noise = diffusion(latents, context, time_embedding)\n            latents = scheduler.denoising_step(timestep, latents, predicted_latent_noise)\n\n        # From latent, go back to Original image\n        images = decoder(latents)\n\n        # Fix colors\n        images = rescale(images, (-1, 1), (0, 255), clamp=True)\n        # (Batch_Size, Channel, Height, Width) -> (Batch_Size, Height, Width, Channel)\n        images = images.to(\"cpu\", torch.uint8).numpy()\n    return images","metadata":{"id":"BADujI8frv2I","execution":{"iopub.status.busy":"2024-11-16T17:57:08.078092Z","iopub.execute_input":"2024-11-16T17:57:08.078493Z","iopub.status.idle":"2024-11-16T17:57:08.091231Z","shell.execute_reply.started":"2024-11-16T17:57:08.078449Z","shell.execute_reply":"2024-11-16T17:57:08.090344Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Training pipeline","metadata":{"id":"pL_d5KNIwrg2"}},{"cell_type":"code","source":"import torch.optim as optim\n#TODO: take configs from stable diffustion github\nscheduler=Scheduler() #!\nencoder=VAEEncoder()\ntokenizer = Tokenizer(opposites)\ntextencoder = TextEncoder()\ndiffusion=Diffusion()\ndecoder=VAEDecoder()\n# textencoder=nn.DataParallel(textencoder)\n# diffusion=nn.parallel.DistributedDataParallel(diffusion)\n# encoder=nn.DataParallel(encoder)\n# decoder=nn.DataParallel(decoder)","metadata":{"id":"rcB4CSu847OF","execution":{"iopub.status.busy":"2024-11-16T17:57:08.092337Z","iopub.execute_input":"2024-11-16T17:57:08.092706Z","iopub.status.idle":"2024-11-16T17:57:12.009502Z","shell.execute_reply.started":"2024-11-16T17:57:08.092666Z","shell.execute_reply":"2024-11-16T17:57:12.008727Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"print(sum(p.numel() for p in encoder.parameters()))\nprint(sum(p.numel() for p in textencoder.parameters()))\nprint(sum(p.numel() for p in diffusion.parameters()))\nprint(sum(p.numel() for p in decoder.parameters()))","metadata":{"id":"YuOICiw_hS-f","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8811077f-8a48-4201-cd5c-eaa6cc2267c7","execution":{"iopub.status.busy":"2024-11-16T17:57:12.010592Z","iopub.execute_input":"2024-11-16T17:57:12.010895Z","iopub.status.idle":"2024-11-16T17:57:12.020640Z","shell.execute_reply.started":"2024-11-16T17:57:12.010863Z","shell.execute_reply":"2024-11-16T17:57:12.019745Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"2148144\n105568\n408647044\n3104343\n","output_type":"stream"}]},{"cell_type":"code","source":"WIDTH = 256\nHEIGHT = 256\nLATENTS_WIDTH = WIDTH // 8\nLATENTS_HEIGHT = HEIGHT // 8\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\ndiffusion.to(device)\nencoder.to(device)\ndecoder.to(device)\ntextencoder.to(device)\nstrength=0.8\ndo_cfg=False\ncfg_scale=7.5\nn_inference_steps=50 #!\nmodels={\"image_encoder\":encoder,\n        \"tokenizer\":tokenizer,\n        \"text_encoder\":textencoder,\n        \"diffusion\":diffusion,\n        \"image_decoder\":decoder}\noptimizers={\n    \"image_encoder\":optim.Adam(encoder.parameters(), lr=4.5e-6),\n    \"text_encoder\":optim.Adam(textencoder.parameters(), lr=5e-4),\n    \"diffusion\":optim.Adam(diffusion.parameters(), lr=2e-6),\n    \"image_decoder\":optim.Adam(decoder.parameters(), lr=4.5e-6)\n}\ndevice=\"cuda\" if torch.cuda.is_available() else \"cpu\"\nidle_device=\"cpu\"","metadata":{"id":"UOBIi6wfBKVn","execution":{"iopub.status.busy":"2024-11-16T17:57:12.021955Z","iopub.execute_input":"2024-11-16T17:57:12.022307Z","iopub.status.idle":"2024-11-16T17:57:12.766819Z","shell.execute_reply.started":"2024-11-16T17:57:12.022268Z","shell.execute_reply":"2024-11-16T17:57:12.766034Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"for original_images_batch, descriptions_batch in dataloader:\n    descriptions_evaluate=descriptions_batch\n    break","metadata":{"execution":{"iopub.status.busy":"2024-11-16T17:57:12.767889Z","iopub.execute_input":"2024-11-16T17:57:12.768190Z","iopub.status.idle":"2024-11-16T17:57:12.819344Z","shell.execute_reply.started":"2024-11-16T17:57:12.768158Z","shell.execute_reply":"2024-11-16T17:57:12.818614Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"# if torch.cuda.device_count() > 1:\n#     print(\"Let's use\", torch.cuda.device_count(), \"GPUs!\")\n#     # dim = 0 [30, xxx] -> [10, ...], [10, ...], [10, ...] on 3 GPUs\n#     diffusion=nn.DataParallel(diffusion)\n# diffusion.to(device)","metadata":{"id":"WCx_ODoZeJsD","colab":{"base_uri":"https://localhost:8080/"},"outputId":"81d4a471-0d62-4641-e86b-f028d138eb16","execution":{"iopub.status.busy":"2024-11-16T17:57:12.820521Z","iopub.execute_input":"2024-11-16T17:57:12.820836Z","iopub.status.idle":"2024-11-16T17:57:12.825031Z","shell.execute_reply.started":"2024-11-16T17:57:12.820804Z","shell.execute_reply":"2024-11-16T17:57:12.824173Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"from tqdm.autonotebook import tqdm\ndef train_one_epoch(diffusion,tokenizer,text_encoder,image_encoder,image_decoder,optimizers,loss_fn,dataloader,scheduler,do_cfg,epoch):\n    loop = tqdm(\n    enumerate(dataloader, 1),\n    total=len(dataloader),\n    desc=f\"Epoch {epoch}: train\",\n    leave=True,\n    )\n    diffusion.train()\n    text_encoder.train()\n    image_encoder.train()\n    image_decoder.train()\n    train_loss = 0.0\n\n    for original_images_batch, descriptions_batch in dataloader:\n        # # Text embeddings, context from prompts creation\n        cond_tokens = tokenizer.tokenize(descriptions_batch)\n        cond_tokens = torch.tensor(cond_tokens, dtype=torch.long, device=device)\n        cond_context = text_encoder(cond_tokens)\n        if do_cfg:\n            uncond_tokens = tokenizer.tokenize(descriptions_batch)\n            uncond_tokens = torch.tensor(uncond_tokens, dtype=torch.long, device=device)\n            uncond_context = text_encoder(uncond_tokens)\n            context = torch.cat([cond_context, uncond_context]) # B*2, 40, 32\n        else:\n            context = cond_context # B, 40, 32\n\n        # move to gpu0\n        original_images_batch=original_images_batch.to(device)\n        context=context.to(device)\n\n        for timestep in range (scheduler.amount_steps-1,0,-scheduler.step_size):\n            scheduler.current_step=timestep\n            # for each denoising step, create timestep embedding from cosine/sine waves\n            time_embedding = get_time_embedding(scheduler.current_step).to(device)\n\n            # # Sample and encode noised images\n            batch_noised_images=scheduler.add_noise(original_images_batch,scheduler.current_step)\n            batch_latent_noised_images=image_encoder(batch_noised_images) # current\n            \n            # Get actual and predicted latent noise\n            predicted_latent_noise = diffusion(batch_latent_noised_images, context, time_embedding)\n            batch_images_previous_prediction_latent = scheduler.denoising_step(timestep, batch_latent_noised_images, predicted_latent_noise)\n            sample_latent_noise=batch_images_previous_prediction_latent-batch_latent_noised_images\n\n            predicted_noise=image_decoder(predicted_latent_noise)\n            sample_noise=image_decoder(sample_latent_noise)\n\n            # MSE between sample noise and predicted noise\n            loss=loss_fn(predicted_noise,sample_noise)\n            train_loss+=loss\n\n            optimizers['text_encoder'].zero_grad()\n            optimizers['image_encoder'].zero_grad()\n            optimizers['diffusion'].zero_grad()\n\n            loss.backward()\n            optimizers['text_encoder'].step()\n            optimizers['image_encoder'].step()\n            optimizers['diffusion'].step()\n\n            #  Only one backpropogation for initial text encoder is needed (occured in timestep=999)\n            context=context.detach()\n\n            # Train decoder with special loss\n            batch_images_previous_prediction=image_decoder(batch_images_previous_prediction_latent.detach())\n#             print(context.shape,batch_images_previous_prediction.shape) [2, 40, 32]) torch.Size([2, 3, 256, 256])\n            decoder_loss=image_context_similarity_loss(batch_images_previous_prediction,context)\n            optimizers['image_decoder'].zero_grad()\n            decoder_loss.backward(retain_graph=True)\n            optimizers['image_decoder'].step()\n            train_loss+=decoder_loss\n            \n        loop.set_postfix(loss=train_loss / (loop.n or 1))\n    return train_loss","metadata":{"id":"wXFppLff4UJ3","execution":{"iopub.status.busy":"2024-11-16T17:57:12.826478Z","iopub.execute_input":"2024-11-16T17:57:12.826951Z","iopub.status.idle":"2024-11-16T17:57:12.842674Z","shell.execute_reply.started":"2024-11-16T17:57:12.826906Z","shell.execute_reply":"2024-11-16T17:57:12.841724Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"from torch.nn import MSELoss\nimport matplotlib.pyplot as plt\nimport torch\n\nfor epoch in range(10):\n    loss=train_one_epoch(diffusion, tokenizer, textencoder, encoder, decoder, optimizers, MSELoss(), dataloader, scheduler, False, epoch)\n    scheduler.reset_parameters()\n    \n    if epoch % 1 == 0:\n        # Save checkpoints\n        torch.save(diffusion.state_dict(), f\"diffusion_epoch_{epoch}.pth\")\n        torch.save(textencoder.state_dict(), f\"textencoder_epoch_{epoch}.pth\")\n        torch.save(encoder.state_dict(), f\"encoder_epoch_{epoch}.pth\")\n        torch.save(decoder.state_dict(), f\"decoder_epoch_{epoch}.pth\")\n        \n        denoised_images = evaluate(diffusion, tokenizer, textencoder, encoder, decoder, n_inference_steps, descriptions_evaluate, scheduler, False)\n        print(epoch, loss)\n        batch_size = denoised_images.shape[0]\n        fig, axes = plt.subplots(1, batch_size, figsize=(15, 5))\n        # Ensure axes is always a list (wrap single axis in a list for batch_size=1)\n        if batch_size == 1:\n            axes = [axes]\n        for i, image in enumerate(denoised_images):\n            # Transpose to (H, W, C) format for NumPy\n            axes[i].imshow(image.transpose(1, 2, 0))\n            axes[i].axis('off')\n        plt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":101,"referenced_widgets":["cbac743b2cb44e0b9881a98a23de6317","37afc165b7334a049e8e50983c188f7b","d22d9f1a7c4c4ef5bd11086ec7a69dc4","1cc04a92d8884d5b8081195bfc2f4cc4","30cc8298bda444e8a1485bb2da0bf2c6","8f5e9e406e3f47bb8c7aad278e67db79","66ae1b5f7189418b906553ec16771f8c","19b201c040c0410e8eddfca6de6788fe","81a57e9b24e74df8b29b2549a59f5750","a8e6d4f457c8446691a41832c9f594f9","eeeed015ee8640cd9b75db39e906bc88"]},"id":"g_Xv4fA3CVDd","outputId":"9e9fe58d-b3a5-426d-84a0-1389031957ed","execution":{"iopub.status.busy":"2024-11-16T17:57:12.843828Z","iopub.execute_input":"2024-11-16T17:57:12.844171Z","iopub.status.idle":"2024-11-16T17:57:19.929736Z","shell.execute_reply.started":"2024-11-16T17:57:12.844140Z","shell.execute_reply":"2024-11-16T17:57:19.928301Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch 0: train:   0%|          | 0/101300 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252a728aa90940f6b80f9335a70b63a6"}},"metadata":{}},{"name":"stdout","text":"torch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 40, 32]) torch.Size([1, 320])\ntorch.Size([2, 4, 32, 32]) torch.Size([2, 4, 32, 32])\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[24], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 6\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdiffusion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtextencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMSELoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mreset_parameters()\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n","Cell \u001b[0;32mIn[23], line 49\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(diffusion, tokenizer, text_encoder, image_encoder, image_decoder, optimizers, loss_fn, dataloader, scheduler, do_cfg, epoch)\u001b[0m\n\u001b[1;32m     46\u001b[0m sample_latent_noise\u001b[38;5;241m=\u001b[39mbatch_images_previous_prediction_latent\u001b[38;5;241m-\u001b[39mbatch_latent_noised_images\n\u001b[1;32m     48\u001b[0m predicted_noise\u001b[38;5;241m=\u001b[39mimage_decoder(predicted_latent_noise)\n\u001b[0;32m---> 49\u001b[0m sample_noise\u001b[38;5;241m=\u001b[39m\u001b[43mimage_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_latent_noise\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# MSE between sample noise and predicted noise\u001b[39;00m\n\u001b[1;32m     52\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(predicted_noise,sample_noise)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[9], line 79\u001b[0m, in \u001b[0;36mVAEDecoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m---> 79\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m0.18125\u001b[39m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","Cell \u001b[0;32mIn[8], line 51\u001b[0m, in \u001b[0;36mVAEResidualBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroupnorm_2(x)\n\u001b[1;32m     50\u001b[0m x\u001b[38;5;241m=\u001b[39mF\u001b[38;5;241m.\u001b[39msilu(x)\n\u001b[0;32m---> 51\u001b[0m x\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m+\u001b[39mresidue\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU "],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 20.00 MiB. GPU ","output_type":"error"}]},{"cell_type":"code","source":"        # # Create initial noised latent image\n#         if not training_mode:\n#             # evaluation mode, generate image from initial pure noise\n#             scheduler.set_inference_mode(scheduler.step_size)\n#             latents_shape = (context.shape[0], 4, LATENTS_HEIGHT, LATENTS_WIDTH) # B, 4, 16, 16 #? what is 4?\n#             batch_latent_noised_images = torch.randn(latents_shape, device=device)\n# if do_cfg:\n#                 # Copy same latent noised image for conditional and uncoditional\n#                 batch_latent_noised_images = batch_latent_noised_images.repeat(2, 1, 1, 1)\n#             if do_cfg:\n#                 #?\n#                 output_cond, output_uncond = predicted_noise.chunk(2)\n#                 predicted_noise = cfg_scale * (output_cond - output_uncond) + output_uncond","metadata":{"id":"CAtE-pcDG19T","execution":{"iopub.status.busy":"2024-11-16T17:57:19.930706Z","iopub.status.idle":"2024-11-16T17:57:19.931048Z","shell.execute_reply.started":"2024-11-16T17:57:19.930881Z","shell.execute_reply":"2024-11-16T17:57:19.930898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}